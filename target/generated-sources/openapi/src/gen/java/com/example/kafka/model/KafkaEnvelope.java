/*
 * Kafka Message Models
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package com.example.kafka.model;

import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.util.StringJoiner;
import java.util.Objects;
import java.util.Map;
import java.util.HashMap;
import com.example.kafka.model.ApplicationAudit;
import com.example.kafka.model.BpmType;
import com.example.kafka.model.CamundaAudit;
import com.example.kafka.model.ErrorType;
import com.example.kafka.model.InstanceDlq;
import com.example.kafka.model.IpeAudit;
import com.example.kafka.model.MessageModelType;
import com.example.kafka.model.TaskDlq;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonSubTypes;
import com.fasterxml.jackson.annotation.JsonTypeInfo;
import com.fasterxml.jackson.annotation.JsonTypeName;
import com.fasterxml.jackson.annotation.JsonValue;
import java.time.OffsetDateTime;
import java.util.Arrays;
import com.fasterxml.jackson.annotation.JsonPropertyOrder;

import com.fasterxml.jackson.core.type.TypeReference;

import java.io.IOException;
import java.util.logging.Level;
import java.util.logging.Logger;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.JsonToken;
import com.fasterxml.jackson.databind.DeserializationContext;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.MapperFeature;
import com.fasterxml.jackson.databind.SerializerProvider;
import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
import com.fasterxml.jackson.databind.annotation.JsonSerialize;
import com.fasterxml.jackson.databind.deser.std.StdDeserializer;
import com.fasterxml.jackson.databind.ser.std.StdSerializer;
import com.example.kafka.JSON;

@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2025-08-05T14:18:54.933188300+01:00[Europe/London]", comments = "Generator version: 7.6.0")
@JsonDeserialize(using = KafkaEnvelope.KafkaEnvelopeDeserializer.class)
@JsonSerialize(using = KafkaEnvelope.KafkaEnvelopeSerializer.class)
public class KafkaEnvelope extends AbstractOpenApiSchema {
    private static final Logger log = Logger.getLogger(KafkaEnvelope.class.getName());

    public static class KafkaEnvelopeSerializer extends StdSerializer<KafkaEnvelope> {
        public KafkaEnvelopeSerializer(Class<KafkaEnvelope> t) {
            super(t);
        }

        public KafkaEnvelopeSerializer() {
            this(null);
        }

        @Override
        public void serialize(KafkaEnvelope value, JsonGenerator jgen, SerializerProvider provider) throws IOException, JsonProcessingException {
            jgen.writeObject(value.getActualInstance());
        }
    }

    public static class KafkaEnvelopeDeserializer extends StdDeserializer<KafkaEnvelope> {
        public KafkaEnvelopeDeserializer() {
            this(KafkaEnvelope.class);
        }

        public KafkaEnvelopeDeserializer(Class<?> vc) {
            super(vc);
        }

        @Override
        public KafkaEnvelope deserialize(JsonParser jp, DeserializationContext ctxt) throws IOException, JsonProcessingException {
            JsonNode tree = jp.readValueAsTree();
            Object deserialized = null;
            boolean typeCoercion = ctxt.isEnabled(MapperFeature.ALLOW_COERCION_OF_SCALARS);
            int match = 0;
            JsonToken token = tree.traverse(jp.getCodec()).nextToken();
            // deserialize ApplicationAudit
            try {
                boolean attemptParsing = true;
                // ensure that we respect type coercion as set on the client ObjectMapper
                if (ApplicationAudit.class.equals(Integer.class) || ApplicationAudit.class.equals(Long.class) || ApplicationAudit.class.equals(Float.class) || ApplicationAudit.class.equals(Double.class) || ApplicationAudit.class.equals(Boolean.class) || ApplicationAudit.class.equals(String.class)) {
                    attemptParsing = typeCoercion;
                    if (!attemptParsing) {
                        attemptParsing |= ((ApplicationAudit.class.equals(Integer.class) || ApplicationAudit.class.equals(Long.class)) && token == JsonToken.VALUE_NUMBER_INT);
                        attemptParsing |= ((ApplicationAudit.class.equals(Float.class) || ApplicationAudit.class.equals(Double.class)) && token == JsonToken.VALUE_NUMBER_FLOAT);
                        attemptParsing |= (ApplicationAudit.class.equals(Boolean.class) && (token == JsonToken.VALUE_FALSE || token == JsonToken.VALUE_TRUE));
                        attemptParsing |= (ApplicationAudit.class.equals(String.class) && token == JsonToken.VALUE_STRING);
                    }
                }
                if (attemptParsing) {
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(ApplicationAudit.class);
                    // TODO: there is no validation against JSON schema constraints
                    // (min, max, enum, pattern...), this does not perform a strict JSON
                    // validation, which means the 'match' count may be higher than it should be.
                    match++;
                    log.log(Level.FINER, "Input data matches schema 'ApplicationAudit'");
                }
            } catch (Exception e) {
                // deserialization failed, continue
                log.log(Level.FINER, "Input data does not match schema 'ApplicationAudit'", e);
            }

            // deserialize CamundaAudit
            try {
                boolean attemptParsing = true;
                // ensure that we respect type coercion as set on the client ObjectMapper
                if (CamundaAudit.class.equals(Integer.class) || CamundaAudit.class.equals(Long.class) || CamundaAudit.class.equals(Float.class) || CamundaAudit.class.equals(Double.class) || CamundaAudit.class.equals(Boolean.class) || CamundaAudit.class.equals(String.class)) {
                    attemptParsing = typeCoercion;
                    if (!attemptParsing) {
                        attemptParsing |= ((CamundaAudit.class.equals(Integer.class) || CamundaAudit.class.equals(Long.class)) && token == JsonToken.VALUE_NUMBER_INT);
                        attemptParsing |= ((CamundaAudit.class.equals(Float.class) || CamundaAudit.class.equals(Double.class)) && token == JsonToken.VALUE_NUMBER_FLOAT);
                        attemptParsing |= (CamundaAudit.class.equals(Boolean.class) && (token == JsonToken.VALUE_FALSE || token == JsonToken.VALUE_TRUE));
                        attemptParsing |= (CamundaAudit.class.equals(String.class) && token == JsonToken.VALUE_STRING);
                    }
                }
                if (attemptParsing) {
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(CamundaAudit.class);
                    // TODO: there is no validation against JSON schema constraints
                    // (min, max, enum, pattern...), this does not perform a strict JSON
                    // validation, which means the 'match' count may be higher than it should be.
                    match++;
                    log.log(Level.FINER, "Input data matches schema 'CamundaAudit'");
                }
            } catch (Exception e) {
                // deserialization failed, continue
                log.log(Level.FINER, "Input data does not match schema 'CamundaAudit'", e);
            }

            // deserialize InstanceDlq
            try {
                boolean attemptParsing = true;
                // ensure that we respect type coercion as set on the client ObjectMapper
                if (InstanceDlq.class.equals(Integer.class) || InstanceDlq.class.equals(Long.class) || InstanceDlq.class.equals(Float.class) || InstanceDlq.class.equals(Double.class) || InstanceDlq.class.equals(Boolean.class) || InstanceDlq.class.equals(String.class)) {
                    attemptParsing = typeCoercion;
                    if (!attemptParsing) {
                        attemptParsing |= ((InstanceDlq.class.equals(Integer.class) || InstanceDlq.class.equals(Long.class)) && token == JsonToken.VALUE_NUMBER_INT);
                        attemptParsing |= ((InstanceDlq.class.equals(Float.class) || InstanceDlq.class.equals(Double.class)) && token == JsonToken.VALUE_NUMBER_FLOAT);
                        attemptParsing |= (InstanceDlq.class.equals(Boolean.class) && (token == JsonToken.VALUE_FALSE || token == JsonToken.VALUE_TRUE));
                        attemptParsing |= (InstanceDlq.class.equals(String.class) && token == JsonToken.VALUE_STRING);
                    }
                }
                if (attemptParsing) {
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(InstanceDlq.class);
                    // TODO: there is no validation against JSON schema constraints
                    // (min, max, enum, pattern...), this does not perform a strict JSON
                    // validation, which means the 'match' count may be higher than it should be.
                    match++;
                    log.log(Level.FINER, "Input data matches schema 'InstanceDlq'");
                }
            } catch (Exception e) {
                // deserialization failed, continue
                log.log(Level.FINER, "Input data does not match schema 'InstanceDlq'", e);
            }

            // deserialize IpeAudit
            try {
                boolean attemptParsing = true;
                // ensure that we respect type coercion as set on the client ObjectMapper
                if (IpeAudit.class.equals(Integer.class) || IpeAudit.class.equals(Long.class) || IpeAudit.class.equals(Float.class) || IpeAudit.class.equals(Double.class) || IpeAudit.class.equals(Boolean.class) || IpeAudit.class.equals(String.class)) {
                    attemptParsing = typeCoercion;
                    if (!attemptParsing) {
                        attemptParsing |= ((IpeAudit.class.equals(Integer.class) || IpeAudit.class.equals(Long.class)) && token == JsonToken.VALUE_NUMBER_INT);
                        attemptParsing |= ((IpeAudit.class.equals(Float.class) || IpeAudit.class.equals(Double.class)) && token == JsonToken.VALUE_NUMBER_FLOAT);
                        attemptParsing |= (IpeAudit.class.equals(Boolean.class) && (token == JsonToken.VALUE_FALSE || token == JsonToken.VALUE_TRUE));
                        attemptParsing |= (IpeAudit.class.equals(String.class) && token == JsonToken.VALUE_STRING);
                    }
                }
                if (attemptParsing) {
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(IpeAudit.class);
                    // TODO: there is no validation against JSON schema constraints
                    // (min, max, enum, pattern...), this does not perform a strict JSON
                    // validation, which means the 'match' count may be higher than it should be.
                    match++;
                    log.log(Level.FINER, "Input data matches schema 'IpeAudit'");
                }
            } catch (Exception e) {
                // deserialization failed, continue
                log.log(Level.FINER, "Input data does not match schema 'IpeAudit'", e);
            }

            // deserialize TaskDlq
            try {
                boolean attemptParsing = true;
                // ensure that we respect type coercion as set on the client ObjectMapper
                if (TaskDlq.class.equals(Integer.class) || TaskDlq.class.equals(Long.class) || TaskDlq.class.equals(Float.class) || TaskDlq.class.equals(Double.class) || TaskDlq.class.equals(Boolean.class) || TaskDlq.class.equals(String.class)) {
                    attemptParsing = typeCoercion;
                    if (!attemptParsing) {
                        attemptParsing |= ((TaskDlq.class.equals(Integer.class) || TaskDlq.class.equals(Long.class)) && token == JsonToken.VALUE_NUMBER_INT);
                        attemptParsing |= ((TaskDlq.class.equals(Float.class) || TaskDlq.class.equals(Double.class)) && token == JsonToken.VALUE_NUMBER_FLOAT);
                        attemptParsing |= (TaskDlq.class.equals(Boolean.class) && (token == JsonToken.VALUE_FALSE || token == JsonToken.VALUE_TRUE));
                        attemptParsing |= (TaskDlq.class.equals(String.class) && token == JsonToken.VALUE_STRING);
                    }
                }
                if (attemptParsing) {
                    deserialized = tree.traverse(jp.getCodec()).readValueAs(TaskDlq.class);
                    // TODO: there is no validation against JSON schema constraints
                    // (min, max, enum, pattern...), this does not perform a strict JSON
                    // validation, which means the 'match' count may be higher than it should be.
                    match++;
                    log.log(Level.FINER, "Input data matches schema 'TaskDlq'");
                }
            } catch (Exception e) {
                // deserialization failed, continue
                log.log(Level.FINER, "Input data does not match schema 'TaskDlq'", e);
            }

            if (match == 1) {
                KafkaEnvelope ret = new KafkaEnvelope();
                ret.setActualInstance(deserialized);
                return ret;
            }
            throw new IOException(String.format("Failed deserialization for KafkaEnvelope: %d classes match result, expected 1", match));
        }

        /**
         * Handle deserialization of the 'null' value.
         */
        @Override
        public KafkaEnvelope getNullValue(DeserializationContext ctxt) throws JsonMappingException {
            throw new JsonMappingException(ctxt.getParser(), "KafkaEnvelope cannot be null");
        }
    }

    // store a list of schema names defined in oneOf
    public static final Map<String, Class<?>> schemas = new HashMap<>();

    public KafkaEnvelope() {
        super("oneOf", Boolean.FALSE);
    }

    public KafkaEnvelope(ApplicationAudit o) {
        super("oneOf", Boolean.FALSE);
        setActualInstance(o);
    }

    public KafkaEnvelope(CamundaAudit o) {
        super("oneOf", Boolean.FALSE);
        setActualInstance(o);
    }

    public KafkaEnvelope(InstanceDlq o) {
        super("oneOf", Boolean.FALSE);
        setActualInstance(o);
    }

    public KafkaEnvelope(IpeAudit o) {
        super("oneOf", Boolean.FALSE);
        setActualInstance(o);
    }

    public KafkaEnvelope(TaskDlq o) {
        super("oneOf", Boolean.FALSE);
        setActualInstance(o);
    }

    static {
        schemas.put("ApplicationAudit", ApplicationAudit.class);
        schemas.put("CamundaAudit", CamundaAudit.class);
        schemas.put("InstanceDlq", InstanceDlq.class);
        schemas.put("IpeAudit", IpeAudit.class);
        schemas.put("TaskDlq", TaskDlq.class);
        JSON.registerDescendants(KafkaEnvelope.class, Collections.unmodifiableMap(schemas));
        // Initialize and register the discriminator mappings.
        Map<String, Class<?>> mappings = new HashMap<String, Class<?>>();
        mappings.put("APPLICATION_AUDIT", ApplicationAudit.class);
        mappings.put("CAMUNDA_AUDIT", CamundaAudit.class);
        mappings.put("INSTANCE_DLQ", InstanceDlq.class);
        mappings.put("IPE_AUDIT", IpeAudit.class);
        mappings.put("TASK_DLQ", TaskDlq.class);
        mappings.put("ApplicationAudit", ApplicationAudit.class);
        mappings.put("CamundaAudit", CamundaAudit.class);
        mappings.put("InstanceDlq", InstanceDlq.class);
        mappings.put("IpeAudit", IpeAudit.class);
        mappings.put("TaskDlq", TaskDlq.class);
        mappings.put("KafkaEnvelope", KafkaEnvelope.class);
        JSON.registerDiscriminator(KafkaEnvelope.class, "messageModelType", mappings);
    }

    @Override
    public Map<String, Class<?>> getSchemas() {
        return KafkaEnvelope.schemas;
    }

    /**
     * Set the instance that matches the oneOf child schema, check
     * the instance parameter is valid against the oneOf child schemas:
     * ApplicationAudit, CamundaAudit, InstanceDlq, IpeAudit, TaskDlq
     *
     * It could be an instance of the 'oneOf' schemas.
     * The oneOf child schemas may themselves be a composed schema (allOf, anyOf, oneOf).
     */
    @Override
    public void setActualInstance(Object instance) {
        if (JSON.isInstanceOf(ApplicationAudit.class, instance, new HashSet<Class<?>>())) {
            super.setActualInstance(instance);
            return;
        }

        if (JSON.isInstanceOf(CamundaAudit.class, instance, new HashSet<Class<?>>())) {
            super.setActualInstance(instance);
            return;
        }

        if (JSON.isInstanceOf(InstanceDlq.class, instance, new HashSet<Class<?>>())) {
            super.setActualInstance(instance);
            return;
        }

        if (JSON.isInstanceOf(IpeAudit.class, instance, new HashSet<Class<?>>())) {
            super.setActualInstance(instance);
            return;
        }

        if (JSON.isInstanceOf(TaskDlq.class, instance, new HashSet<Class<?>>())) {
            super.setActualInstance(instance);
            return;
        }

        throw new RuntimeException("Invalid instance type. Must be ApplicationAudit, CamundaAudit, InstanceDlq, IpeAudit, TaskDlq");
    }

    /**
     * Get the actual instance, which can be the following:
     * ApplicationAudit, CamundaAudit, InstanceDlq, IpeAudit, TaskDlq
     *
     * @return The actual instance (ApplicationAudit, CamundaAudit, InstanceDlq, IpeAudit, TaskDlq)
     */
    @Override
    public Object getActualInstance() {
        return super.getActualInstance();
    }

    /**
     * Get the actual instance of `ApplicationAudit`. If the actual instance is not `ApplicationAudit`,
     * the ClassCastException will be thrown.
     *
     * @return The actual instance of `ApplicationAudit`
     * @throws ClassCastException if the instance is not `ApplicationAudit`
     */
    public ApplicationAudit getApplicationAudit() throws ClassCastException {
        return (ApplicationAudit)super.getActualInstance();
    }

    /**
     * Get the actual instance of `CamundaAudit`. If the actual instance is not `CamundaAudit`,
     * the ClassCastException will be thrown.
     *
     * @return The actual instance of `CamundaAudit`
     * @throws ClassCastException if the instance is not `CamundaAudit`
     */
    public CamundaAudit getCamundaAudit() throws ClassCastException {
        return (CamundaAudit)super.getActualInstance();
    }

    /**
     * Get the actual instance of `InstanceDlq`. If the actual instance is not `InstanceDlq`,
     * the ClassCastException will be thrown.
     *
     * @return The actual instance of `InstanceDlq`
     * @throws ClassCastException if the instance is not `InstanceDlq`
     */
    public InstanceDlq getInstanceDlq() throws ClassCastException {
        return (InstanceDlq)super.getActualInstance();
    }

    /**
     * Get the actual instance of `IpeAudit`. If the actual instance is not `IpeAudit`,
     * the ClassCastException will be thrown.
     *
     * @return The actual instance of `IpeAudit`
     * @throws ClassCastException if the instance is not `IpeAudit`
     */
    public IpeAudit getIpeAudit() throws ClassCastException {
        return (IpeAudit)super.getActualInstance();
    }

    /**
     * Get the actual instance of `TaskDlq`. If the actual instance is not `TaskDlq`,
     * the ClassCastException will be thrown.
     *
     * @return The actual instance of `TaskDlq`
     * @throws ClassCastException if the instance is not `TaskDlq`
     */
    public TaskDlq getTaskDlq() throws ClassCastException {
        return (TaskDlq)super.getActualInstance();
    }



  /**
   * Convert the instance into URL query string.
   *
   * @return URL query string
   */
  public String toUrlQueryString() {
    return toUrlQueryString(null);
  }

  /**
   * Convert the instance into URL query string.
   *
   * @param prefix prefix of the query string
   * @return URL query string
   */
  public String toUrlQueryString(String prefix) {
    String suffix = "";
    String containerSuffix = "";
    String containerPrefix = "";
    if (prefix == null) {
      // style=form, explode=true, e.g. /pet?name=cat&type=manx
      prefix = "";
    } else {
      // deepObject style e.g. /pet?id[name]=cat&id[type]=manx
      prefix = prefix + "[";
      suffix = "]";
      containerSuffix = "]";
      containerPrefix = "[";
    }

    StringJoiner joiner = new StringJoiner("&");

    if (getActualInstance() instanceof TaskDlq) {
        if (getActualInstance() != null) {
          joiner.add(((TaskDlq)getActualInstance()).toUrlQueryString(prefix + "one_of_0" + suffix));
        }
        return joiner.toString();
    }
    if (getActualInstance() instanceof InstanceDlq) {
        if (getActualInstance() != null) {
          joiner.add(((InstanceDlq)getActualInstance()).toUrlQueryString(prefix + "one_of_1" + suffix));
        }
        return joiner.toString();
    }
    if (getActualInstance() instanceof IpeAudit) {
        if (getActualInstance() != null) {
          joiner.add(((IpeAudit)getActualInstance()).toUrlQueryString(prefix + "one_of_2" + suffix));
        }
        return joiner.toString();
    }
    if (getActualInstance() instanceof CamundaAudit) {
        if (getActualInstance() != null) {
          joiner.add(((CamundaAudit)getActualInstance()).toUrlQueryString(prefix + "one_of_3" + suffix));
        }
        return joiner.toString();
    }
    if (getActualInstance() instanceof ApplicationAudit) {
        if (getActualInstance() != null) {
          joiner.add(((ApplicationAudit)getActualInstance()).toUrlQueryString(prefix + "one_of_4" + suffix));
        }
        return joiner.toString();
    }
    return null;
  }

}

